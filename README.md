# EFO-1-QA Benchmark for First Order Query Estimation on Knowledge Graphs

This repository contains an entire pipeline for the EFO-1-QA benchmark. EFO-1 stands for the Existential First Order Queries with Single Free Varibales. The related paper has been submitted to the NeurIPS 2021 track on dataset and benchmark. [OpenReview Link](https://openreview.net/forum?id=pX4x8f6Km5T).

## The pipeline overview.

![alt text](figures/pipeline.png)

1. **Query type generation and normalization** The query types are generated by the DFS iteration of the context free grammar with the bounded negation hypothesis. The generated types are also normalized to several normal forms
2. **Query grounding and answer sampling** The queries are grounded on specific knowledge graphs and the answers that are non-trivial are sampled.
3. **Model training and estimation** We train and evaluate the specific query structure 

## Query type generation and normalization
The OpsTree is represented in the nested objects of `FirstOrderSetQuery` class in `fol/foq_v2.py`. 
We first generate the specific OpsTree and then store then by the `formula` property of `FirstOrderSetQuery`.

The OpsTree is generated by `binary_formula_iterator` in `fol/foq_v2.py`. The overall process is managed in `formula_generation.py`.

To generate the formula, just run
```bash
python formula_generation.py
```

Then the file formula csv is generated in the `outputs` folder.
In this paper, we use the file in `outputs/test_generated_formula_anchor_node=3.csv`

## Query grounding and answer sampling

We first prepare the KG data and then run the sampling code

The KG data (FB15k, FB15k-237, NELL995) should be put into under 'data/' folder. We use the [data](http://snap.stanford.edu/betae/KG_data.zip) provided in the [KGReasoning](https://github.com/snap-stanford/KGReasoning).

The structure of the data folder should be at least

```
data
	|---FB15k-237-betae
	|---FB15k-betae
	|---NELL-betae	
```

Then we can run the sampling code by
```
python benchmark_sampling.py
```



## Model training and estimation



**Models**

- [x] [BetaE](https://arxiv.org/abs/2010.11465)
- [x] [Query2box](https://arxiv.org/abs/2002.05969)
- [x] [NewLook](http://tonghanghang.org/pdfs/kdd21_newlook.pdf)
- [x] [LogicE](https://arxiv.org/abs/2103.00418)

**Examples**

Please refer to the `examples.sh` for the scripts of all 3 models on all 3 datasets.
The detailed 
If you want to train models:

```bash
python main.py --config config/default.yaml
python main.py --config config/Query2Box.yaml
python main.py --config config/NewLook.yaml
python main.py --config config/Logic.yaml
```

If you need to evaluate on the EFO-1-QA benchmark, be sure to load from a model checkpoint, it can de downloaded
from here():

```bash
python main.py --config config/benchmark_beta.yaml --checkpoint_path ckpt/FB15k/Beta_full
python main.py --config config/benchmark_NewLook.yaml --checkpoint_path ckpt/FB15k/NLK_full --load_step 450000
python main.py --config config/benchmark_Logic.yaml --checkpoint_path ckpt/FB15k/Logic_full --load_step 450000
```

**Citations**

If you use this repo, please cite the following paper.

