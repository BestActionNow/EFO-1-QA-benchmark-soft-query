{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.std import trange\n",
    "\n",
    "from data_helper import (SingledirectionalOneShotIterator, TestDataset,\n",
    "                         TrainDataset)\n",
    "from fol import BetaEstimator, BoxEstimator, TransEEstimator, parse_foq_formula\n",
    "from fol.base import beta_query\n",
    "from util import (Writer, load_graph, load_task_manager, read_from_yaml, read_indexing,\n",
    "                  set_global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "def train_step(model, opt, dataloader, device):\n",
    "    iterator = iter(dataloader)\n",
    "    # list of tuple, [0] is query, [1] ans, [2] beta_name\n",
    "    batch_flattened_query = next(iterator)\n",
    "    all_loss = torch.tensor(0, dtype=torch.float)\n",
    "    opt.zero_grad()  # TODO: parallelize query\n",
    "    # A dict with key of beta_name, value= list of queries\n",
    "    query_dict = collections.defaultdict(list)\n",
    "    ans_dict = collections.defaultdict(list)\n",
    "    for idx in range(len(batch_flattened_query[0])):\n",
    "        query, ans, beta_name = batch_flattened_query[0][idx], \\\n",
    "                                batch_flattened_query[1][idx], \\\n",
    "                                    batch_flattened_query[2][idx]\n",
    "        query_dict[beta_name].append(query)\n",
    "        ans_dict[beta_name].append(ans)\n",
    "    for beta_name in query_dict:\n",
    "        meta_formula = beta_query[beta_name]\n",
    "        query_instance = parse_foq_formula(meta_formula)\n",
    "        for query in query_dict[beta_name]:\n",
    "            query_instance.additive_ground(query)\n",
    "        pred = query_instance.embedding_estimation(estimator=model, device=device)\n",
    "        query_loss = model.criterion(pred, ans_dict[beta_name])\n",
    "        all_loss += query_loss\n",
    "    loss = all_loss.mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    log = {\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "    return log\n",
    "\n",
    "\n",
    "def eval_step(model, dataloader, device):\n",
    "    logs = collections.defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_flattened_query in tqdm.tqdm(dataloader, disable=True):\n",
    "            # A dict with key of beta_name, value= list of queries\n",
    "            query_dict = collections.defaultdict(list)\n",
    "            easy_ans_dict = collections.defaultdict(list)\n",
    "            hard_ans_dict = collections.defaultdict(list)\n",
    "            for idx in range(len(batch_flattened_query[0])):\n",
    "                query, easy_answer, hard_answer, beta_name = batch_flattened_query[0][idx], batch_flattened_query[1][\n",
    "                    idx], \\\n",
    "                                                             batch_flattened_query[2][idx], batch_flattened_query[3][\n",
    "                                                                 idx]\n",
    "                query_dict[beta_name].append(query)\n",
    "                easy_ans_dict[beta_name].append(easy_answer)\n",
    "                hard_ans_dict[beta_name].append(hard_answer)\n",
    "        for beta_name in query_dict:\n",
    "            meta_formula = beta_query[beta_name]\n",
    "            query_instance = parse_foq_formula(meta_formula)\n",
    "            for query in query_dict[beta_name]:\n",
    "                query_instance.additive_ground(query)\n",
    "            pred = query_instance.embedding_estimation(estimator=model, device=device)\n",
    "            all_entity_loss = model.compute_all_entity_logit(\n",
    "                pred)  # batch*nentity\n",
    "            argsort = torch.argsort(all_entity_loss, dim=1, descending=True)\n",
    "            ranking = argsort.clone().to(torch.float)\n",
    "            #  create a new torch Tensor for batch_entity_range\n",
    "            if device != torch.device('cpu'):\n",
    "                ranking = ranking.scatter_(\n",
    "                    1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1).to(\n",
    "                        device))\n",
    "            else:\n",
    "                ranking = ranking.scatter_(\n",
    "                    1, argsort, torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0],\n",
    "                                                                                   1))\n",
    "            # achieve the ranking of all entities\n",
    "            for i in range(all_entity_loss.shape[0]):\n",
    "                easy_ans = easy_ans_dict[beta_name][i]\n",
    "                hard_ans = hard_ans_dict[beta_name][i]\n",
    "                num_hard = len(hard_ans)\n",
    "                num_easy = len(easy_ans)\n",
    "                assert len(set(hard_ans).intersection(set(easy_ans))) == 0\n",
    "                # only take those answers' rank\n",
    "                cur_ranking = ranking[idx, list(easy_ans) + list(hard_ans)]\n",
    "                cur_ranking, indices = torch.sort(cur_ranking)\n",
    "                masks = indices >= num_easy\n",
    "                if device != torch.device('cpu'):\n",
    "                    answer_list = torch.arange(\n",
    "                        num_hard + num_easy).to(torch.float).to(device)\n",
    "                else:\n",
    "                    answer_list = torch.arange(\n",
    "                        num_hard + num_easy).to(torch.float)\n",
    "                cur_ranking = cur_ranking - answer_list + 1\n",
    "                # filtered setting: +1 for start at 0, -answer_list for ignore other answers\n",
    "\n",
    "                cur_ranking = cur_ranking[masks]\n",
    "                # only take indices that belong to the hard answers\n",
    "                mrr = torch.mean(1. / cur_ranking).item()\n",
    "                h1 = torch.mean((cur_ranking <= 1).to(torch.float)).item()\n",
    "                h3 = torch.mean((cur_ranking <= 3).to(torch.float)).item()\n",
    "                h10 = torch.mean(\n",
    "                    (cur_ranking <= 10).to(torch.float)).item()\n",
    "                logs[beta_name].append({\n",
    "                    'MRR': mrr,\n",
    "                    'HITS1': h1,\n",
    "                    'HITS3': h3,\n",
    "                    'HITS10': h10,\n",
    "                    'num_hard_answer': num_hard,\n",
    "                })\n",
    "    return logs\n",
    "\n",
    "\n",
    "# def training(model, opt, train_iterator, valid_iterator, test_iterator, writer, **train_cfg):\n",
    "#     lr = train_cfg['learning_rate']\n",
    "#     with tqdm.trange(train_cfg['steps']) as t:\n",
    "#         for step in t:\n",
    "#             log = train_step(model, opt, train_iterator, writer)\n",
    "#             t.set_postfix({'loss': log['loss']})\n",
    "#             if step % train_cfg['evaluate_every_steps'] and step > 0:\n",
    "#                 eval_step(model, valid_iterator, 'valid', writer, **train_cfg)\n",
    "#                 eval_step(model, test_iterator, 'test', writer, **train_cfg)\n",
    "\n",
    "#             if step >= train_cfg['warm_up_steps']:\n",
    "#                 lr /= 5\n",
    "#                 # logging\n",
    "#                 opt = torch.optim.Adam(\n",
    "#                     filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#                     lr=lr\n",
    "#                 )\n",
    "#                 train_cfg['warm_up_steps'] *= 1.5\n",
    "#             if step % train_cfg['save_every_steps']:\n",
    "#                 pass\n",
    "#             if step % train_cfg['log_every_steps']:\n",
    "#                 pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] config loaded\n",
      "{'action': 'train',\n",
      " 'cuda': 7,\n",
      " 'data': {'cpu': 10, 'data_folder': 'data/FB15k-237-betae', 'type': 'beta'},\n",
      " 'estimator': {'beta': {'entity_dim': 400,\n",
      "                        'evaluate_union': 'DM',\n",
      "                        'gamma': 60,\n",
      "                        'hidden_dim': 1600,\n",
      "                        'num_layers': 2,\n",
      "                        'relation_dim': 400},\n",
      "               'box': {'center_reg': 0.02,\n",
      "                       'entity_dim': 400,\n",
      "                       'gamma': 60,\n",
      "                       'offset_activation': 'None',\n",
      "                       'relation_dim': 400},\n",
      "               'dm': {'entity_dim': 2,\n",
      "                      'hidden_dim': 1600,\n",
      "                      'num_layers': 2,\n",
      "                      'relation_dim': 2},\n",
      "               'embedding': 'beta'},\n",
      " 'evaluate': {'batch_size': 100,\n",
      "              'meta_queries': ['1p', '2p', '2i', '3i'],\n",
      "              'print': True},\n",
      " 'seed': 0,\n",
      " 'train': {'batch_size': 512,\n",
      "           'checkpoint_every': 50000,\n",
      "           'evaluate_every_steps': 30000,\n",
      "           'learning_rate': 0.0001,\n",
      "           'log_every_steps': 100,\n",
      "           'meta_queries': ['1p', '2p', '2i'],\n",
      "           'negative_sample_size': 128,\n",
      "           'steps': 100000,\n",
      "           'warm_up_steps': 1000}}\n",
      "[main] loading the data\n"
     ]
    }
   ],
   "source": [
    "# parse args and load config\n",
    "configure = read_from_yaml('config/default.yaml')\n",
    "print(\"[main] config loaded\")\n",
    "pprint(configure)\n",
    "\n",
    "# initialize my log writer\n",
    "case_name = 'dev/default'\n",
    "writer = Writer(case_name=case_name, config=configure, log_path='log')\n",
    "# writer = SummaryWriter('./logs-debug/unused-tb')\n",
    "\n",
    "# initialize environments\n",
    "set_global_seed(configure.get('seed', 0))\n",
    "if configure.get('cuda', -1) >= 0 and torch.cuda.is_available():\n",
    "    device = torch.device('cuda:{}'.format(configure['cuda']))\n",
    "    # logging.info('Device use cuda: %s' % configure['cuda'])\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# prepare the procedure configs\n",
    "train_config = configure['train']\n",
    "train_config['device'] = device\n",
    "eval_config = configure['evaluate']\n",
    "eval_config['device'] = device\n",
    "\n",
    "# load the data\n",
    "print(\"[main] loading the data\")\n",
    "data_folder = configure['data']['data_folder']\n",
    "entity_dict, relation_dict, id2ent, id2rel = read_indexing(data_folder)\n",
    "n_entity, n_relation = len(entity_dict), len(relation_dict)\n",
    "projection_train, reverse_train = load_graph(\n",
    "    os.path.join(data_folder, 'train.txt'), entity_dict, relation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] load training data\n",
      "[data] load query from file data/FB15k-237-betae/train_1p.csv\n",
      "load from existed files\n",
      "[data] load query from file data/FB15k-237-betae/train_2p.csv\n",
      "load from existed files\n",
      "[data] load query from file data/FB15k-237-betae/train_2i.csv\n",
      "load from existed files\n"
     ]
    }
   ],
   "source": [
    "if 'train' in configure['action']:\n",
    "    print(\"[main] load training data\")\n",
    "    tasks = load_task_manager(\n",
    "        configure['data']['data_folder'], 'train', train_config['meta_queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "model_name = configure['estimator']['embedding']\n",
    "model_params = configure['estimator'][model_name]\n",
    "model_params['n_entity'], model_params['n_relation'] = n_entity, n_relation\n",
    "model_params['device'] = device\n",
    "model_params['negative_sample_size'] = train_config['negative_sample_size']\n",
    "if model_name == 'beta':\n",
    "    model = BetaEstimator(**model_params)\n",
    "elif model_name == 'Box':\n",
    "    model = BoxEstimator(**model_params)\n",
    "model.to(device)\n",
    "\n",
    "lr = train_config['learning_rate']\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move projection object in 139713929685072 to device cuda:7\n",
      "move variable object in 139713929685328 to device cuda:7\n"
     ]
    }
   ],
   "source": [
    "tasks[0].query_instance.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks[0].query_instance.trelations.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139713929685072"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(tasks[0].query_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d0f2889cc596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_estimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Project/FirstOrderQueryEstimation/data_helper.py\u001b[0m in \u001b[0;36mbatch_estimation\u001b[0;34m(self, estimator, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m             batch_embedding = self.query_instance.embedding_estimation(\n\u001b[1;32m     65\u001b[0m                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 batch_indices=batch_indices)\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbatch_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/FirstOrderQueryEstimation/fol/foq.py\u001b[0m in \u001b[0;36membedding_estimation\u001b[0;34m(self, estimator, batch_indices, device)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrelations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0moperand_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperand_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_estimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_projection_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperand_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/FirstOrderQueryEstimation/fol/appfoq.py\u001b[0m in \u001b[0;36mget_projection_embedding\u001b[0;34m(self, proj_ids, emb)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_projection_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mrel_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mpro_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpro_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "a, b = tasks[0].batch_estimation(model, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with trange(1, train_config['steps']+1) as t:\n",
    "    for step in t:\n",
    "        # basic training step\n",
    "        if train_dataloader:\n",
    "            if step >= train_config['warm_up_steps']:\n",
    "                lr /= 5\n",
    "                # logging\n",
    "                opt = torch.optim.Adam(\n",
    "                    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    lr=lr\n",
    "                )\n",
    "                train_config['warm_up_steps'] *= 1.5\n",
    "            _log = train_step(model, opt, train_dataloader, device)\n",
    "            _log['step'] = step\n",
    "            if step % train_config['log_every_steps'] == 0:\n",
    "                writer.append_trace('train', _log)\n",
    "\n",
    "        if step % train_config['evaluate_every_steps'] == 0 or step == train_config['evaluate_every_steps']:\n",
    "            if train_dataloader:\n",
    "                _log = eval_step(model, train_dataloader, device)\n",
    "                _log['step'] = step\n",
    "                writer.append_trace('eval_train', _log)\n",
    "\n",
    "            if valid_dataloader:\n",
    "                _log = eval_step(model, valid_dataloader, device)\n",
    "                _log['step'] = step\n",
    "                writer.append_trace('eval_valid', _log)\n",
    "\n",
    "            if test_dataloader:\n",
    "                _log = eval_step(model, test_dataloader, device)\n",
    "                _log['step'] = step\n",
    "                writer.append_trace('eval_test', _log)\n",
    "\n",
    "        if step % train_config['evaluate_every_steps'] == 0:\n",
    "            writer.save_model(model, step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
